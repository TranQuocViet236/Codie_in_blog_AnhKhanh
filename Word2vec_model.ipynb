{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec_model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOX2NElRTGO5tnFoaF7AaeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranQuocViet236/Codie_in_blog_AnhKhanh/blob/master/Word2vec_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX3CIaDTO8xC"
      },
      "source": [
        "'''\n",
        "One-hot vector: [0,0,...,1,...,0,0]\n",
        "index of the term has 1's value is index of that word in a set of words\n",
        "'''\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#Assign indexes for classes using LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "words = ['anh', 'em', 'gia đình', 'bạn bè', 'anh', 'em']\n",
        "# le.fit(words)\n",
        "# print(f'Class of words: {le.classes_}')\n",
        "#\n",
        "# #Convert to number:\n",
        "# x = le.transform(words)\n",
        "# print(f'Convert to number: {x}')\n",
        "\n",
        "num_words = le.fit_transform(words)\n",
        "print(num_words)\n",
        "\n",
        "print(f'Invert into classes: {le.inverse_transform(num_words)}')\n",
        "\n",
        "print(len(le.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR0E-FvSPEDy"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "classes_indices = list(zip(le.classes_, np.arange(len(le.classes_))))\n",
        "print(f'Classes_indices: {classes_indices}')\n",
        "\n",
        "ohe.fit(classes_indices)\n",
        "\n",
        "print(f'One-hot categories and indices: {ohe.categories_}')\n",
        "\n",
        "#Convert list words into One-hot\n",
        "words_indices = list(zip(words, num_words))\n",
        "print('Words and corresponding indices: ', words_indices)\n",
        "\n",
        "one_hot = ohe.transform(words_indices).toarray()\n",
        "\n",
        "print('Transform words into one-hot matrices: \\n', one_hot)\n",
        "\n",
        "print('Inverse transform to catefories from one hot matrix:\\n', ohe.inverse_transform(one_hot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jG5gUvcWcA7"
      },
      "source": [
        "WORD EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfvfFC0kPwlg"
      },
      "source": [
        "!pip install  underthesea\n",
        "import scipy.linalg as ln\n",
        "import numpy as np\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "sentence = 'Khoa học dữ liệu mà một lĩnh vực đòi hỏi kiến thức về toán và lập trình'\n",
        "token = word_tokenize(sentence)\n",
        "print('tokenization of sentences: ', token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1RifRX0YEv2"
      },
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "#Create coherence matrix into sparse by declaring non-zero position of x and y axes\n",
        "row = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
        "col = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "data = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "X = coo_matrix((data, (row, col)), shape = (15,15)).toarray()\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wx4DzecalY6"
      },
      "source": [
        "# Perform egeneracy analysis\n",
        "U, S_diag, V = ln.svd(X)\n",
        "print('Shape of U', U.shape)\n",
        "print('Length of diagonal: ', len(S_diag))\n",
        "print('Shape of V: ', V.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSxvNMacOaX"
      },
      "source": [
        "import numpy as np\n",
        "S_truncate = np.zeros((6,15))\n",
        "\n",
        "np.fill_diagonal(S_truncate, S_diag[:6])\n",
        "print('S truncate: \\n', S_truncate)\n",
        "print('Word Embedding 6 dimensionality: \\n ', np.dot(S_truncate, V))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3V4GGenR8bX"
      },
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwUYt68ITlt9"
      },
      "source": [
        "def auto_encoder(input_unit, hidden_unit):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(input_unit, input_shape= (15,), activation='relu'))\n",
        "  model.add(Dense(hidden_unit, activation='relu'))\n",
        "  model.add(Dense(input_unit, activation = 'softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer= Adam(), metrics=['acc'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiu4HcPUVD2u"
      },
      "source": [
        "model_auto = auto_encoder(input_unit=15, hidden_unit=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDFvgwx3VNEO"
      },
      "source": [
        "model_auto.fit(X, X, epochs=5, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymxZZCQ6VXy6"
      },
      "source": [
        "embedding_matrix = model_auto.layers[2].get_weights()[0]\n",
        "bias = model_auto.layers[2].get_weights()[1]\n",
        "\n",
        "print('Shape of embedding_matrix: ', embedding_matrix.shape)\n",
        "print('Embedding_matrix: \\n', embedding_matrix)\n",
        "print('Shape of bias matrix: ', bias.shape)\n",
        "print('Bias matrix: \\n', bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Mhh6OPW00U"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine(x,y):\n",
        "  cos_sim = np.dot(x,y)/(norm(x)*norm(y))\n",
        "  return cos_sim\n",
        "# Vector represents 'khoa học'\n",
        "e0 = list(embedding_matrix[:,0])\n",
        "e1 = list(embedding_matrix[:, 1])\n",
        "\n",
        "#Quan hệ tương qua ngữ nghĩa giữa từ khoa học và dữ liệu\n",
        "cosine(e0, e1)\n",
        "\n",
        "print(cosine(e0,e1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGBjeEtXZQyn"
      },
      "source": [
        "# Từ có khoản cách lớn nhất với từ khoa học theo thứ tự\n",
        "cosines = [cosine(e0, embedding_matrix[:,i]) for i in np.arange(15)]\n",
        "\n",
        "print('cosine: ', cosines)\n",
        "np.argsort([cosine(e0, embedding_matrix[:,i]) for i in np.arange(15)])[::-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldnqff7BaLcu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS4kFyIrfpMZ"
      },
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "import nltk\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "norm_bible = gutenberg.sents('/content/gdrive/MyDrive/ML/Embedding_word2vec/bible-kjv.txt')\n",
        "norm_bible = [' '.join(doc) for doc in norm_bible]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHIGZtkhWFn"
      },
      "source": [
        "print(norm_bible)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl4RBZ8GhccL"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt1TIAHohocE"
      },
      "source": [
        "word2id = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1P270Pkh319"
      },
      "source": [
        "word2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xisr5MdPh5dn"
      },
      "source": [
        "# Build vocabulary of unique words\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k,v in word2id.items()}\n",
        "vocab_size = len(word2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozIS-F9uiQ4e"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXwJNMNiZOI"
      },
      "source": [
        "print('Vocab sample: ', list(word2id.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7j9kXVTijcO"
      },
      "source": [
        "#Mã hóa toàn bộ các câu văn bằng văn bản\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rjAPN2GjAlp"
      },
      "source": [
        "print(wids[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSqdIl6wjVz3"
      },
      "source": [
        "# Xác định context -> target\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "  context_length = window_size*2\n",
        "  for words in corpus:\n",
        "    sentence_length = len(words)\n",
        "    for index, word in enumerate(words):\n",
        "      context_words = []\n",
        "      label_word = []\n",
        "\n",
        "      #Start index of context\n",
        "      start = index - window_size\n",
        "      #End index of context \n",
        "      end = index + window_size +1\n",
        "      #List of context_words\n",
        "      context_words.append([words[i] for i in range(start, end) if 0 <=i< sentence_length and i != index])\n",
        "      label_word.append(word)\n",
        "      x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
        "\n",
        "      y = np_utils.to_categorical(label_word, vocab_size)\n",
        "\n",
        "      yield(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI50AiWflQBt"
      },
      "source": [
        "i = 0\n",
        "window_size = 2 # context window size\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdPSp-6poQ9X"
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "embed_size = 100\n",
        "\n",
        "#Build CBOW architecture\n",
        "\n",
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim= vocab_size, output_dim= embed_size, input_length= window_size*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1),output_shape=(embed_size)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer= 'rmsprop')\n",
        "print(cbow.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLFQbPfYvtuL"
      },
      "source": [
        "print('Number of window: ', len(wids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8gA2caov5TF"
      },
      "source": [
        "for epoch in range(1,6):\n",
        "  loss = 0\n",
        "  i = 0\n",
        "  for x,y in generate_context_word_pairs(corpus=wids[:100], window_size=window_size, vocab_size=vocab_size):\n",
        "    i += 1\n",
        "    loss += cbow.train_on_batch(x,y)\n",
        "\n",
        "    if i % 500 ==0:\n",
        "      print(f'Processed {i} (context,  word) pairs')\n",
        "\n",
        "  print('Epoch:', epoch, '\\tloss: ', loss)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy-vovPgw2iB"
      },
      "source": [
        "import pandas as pd\n",
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjix8Je3yPg2"
      },
      "source": [
        "#visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9HmaX__rqAS"
      },
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "#generate skip-grams\n",
        "skip_grams = [skipgrams(wids, vocabulary_size= vocab_size, window_size= window_size ) for wid in wids[:100] ]\n",
        "\n",
        "#view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "    id2word[pairs[i][0]], pairs[i][0],\n",
        "    id2word[pairs[i][1]], pairs[i][1],\n",
        "    labels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q_GjjQL-CW2"
      },
      "source": [
        "# Build neron network\n",
        "\n",
        "from keras.layers import Input, Dot, dot, concatenate\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential, Model, Input\n",
        "\n",
        "#Build skip-gram architecture\n",
        "word_input = Input(shape = (1,))\n",
        "word_embed = Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1, name = 'word_embedding')(word_input)\n",
        "word_output = Reshape((embed_size, ))(word_embed)\n",
        "word_model = Model(word_input, word_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxRr_oRKAtS0"
      },
      "source": [
        "print('word_model: \\n', word_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXhs-b7ZBS8o"
      },
      "source": [
        "context_input = Input(shape = (1,))\n",
        "context_embed = Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1, name = 'context_embedding')(context_input)\n",
        "context_output = Reshape((embed_size,))(context_embed)\n",
        "context_model = Model(context_input, context_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb_c8bd3DQRT"
      },
      "source": [
        "print('context_model: \\n', context_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOHs54P3D3MI"
      },
      "source": [
        "concate = dot([word_output, context_output], axes = -1)\n",
        "dense = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(concate)\n",
        "model = Model(inputs = [word_input, context_input], outputs = dense)\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdjH3qF8EMb9"
      },
      "source": [
        "#view model summary \n",
        "print('model merge word and context: \\n', model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nbWHy5-EYRI"
      },
      "source": [
        "#Visualize model structure\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model,show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOCyD1vwFf5e"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#BIỂU DIỄN T-SNE: giảm chiều dữ liệu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTu2_DUjFuNf"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mTOKBybGgCW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}